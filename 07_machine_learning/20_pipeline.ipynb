{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline in Machine Learning\n",
    "\n",
    "In machine learning, a pipeline is a sequence of data processing steps that are chained together to automate and streamline the machine learning workflow. A pipeline allows you to combine multiple data preprocessing and model training steps into a single object, making it easier to organize and manage your machine learning code.\n",
    "\n",
    "> **Here are the key components of a pipeline:**\n",
    "`Data Preprocessing Steps:` Pipelines typically start with data preprocessing steps, such as feature scaling, feature encoding, handling missing values, or dimensionality reduction. These steps ensure that the data is in the appropriate format and quality for model training.\n",
    "\n",
    "`Model Training:` After the data preprocessing steps, the pipeline includes the training of a machine learning model. This can be a classifier for classification tasks, a regressor for regression tasks, or any other type of model depending on the problem at hand.\n",
    "\n",
    "`Model Evaluation:` Once the model is trained, the pipeline often incorporates steps for evaluating its performance. This may involve metrics calculation, cross-validation, or any other evaluation technique to assess the model's effectiveness.\n",
    "\n",
    "`Predictions:` After the model has been evaluated, the pipeline allows you to make predictions on new, unseen data using the trained model. This step applies the same preprocessing steps used during training to the new data before generating predictions.\n",
    "\n",
    "> **The main advantages of using pipelines in machine learning are:**\n",
    "`Simplified Workflow:` Pipelines provide a clean and organized structure for defining and managing the sequence of steps involved in machine learning tasks. This makes it easier to understand, modify, and reproduce the workflow.\n",
    "\n",
    "`Avoiding Data Leakage:` Pipelines ensure that data preprocessing steps are applied consistently to both the training and testing data, preventing data leakage that could lead to biased or incorrect results.\n",
    "\n",
    "`Streamlined Model Deployment:` Pipelines allow you to encapsulate the entire workflow, including data preprocessing and model training, into a single object. This simplifies the deployment of your machine learning model, as the same pipeline can be applied to new data without the need to reapply each individual step.\n",
    "\n",
    "`Hyperparameter Tuning:` Pipelines can be combined with techniques like grid search or randomized search for hyperparameter tuning. This allows you to efficiently explore different combinations of hyperparameters for your models.\n",
    "______________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Summary:**\n",
    "\n",
    "Overall, pipelines are a powerful tool for managing and automating the machine learning workflow, promoting code reusability, consistency, and efficiency. They help streamline the development and deployment of machine learning models, making it easier to iterate and experiment with different approaches.\n",
    "\n",
    "Here's an example of using a pipeline on the Titanic dataset to preprocess the data, train a model, and make predictions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7877094972067039\n"
     ]
    }
   ],
   "source": [
    "# import all the nessesary libraries \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# load the dataset \n",
    "titanic_data = sns.load_dataset('titanic')\n",
    "\n",
    "# selecting the features and target or label variables\n",
    "X = titanic_data[['pclass', 'sex', 'age', 'fare', 'embarked']]\n",
    "y = titanic_data['survived']\n",
    "\n",
    "# splitting the dataset into training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y , test_size=0.2, random_state=42)\n",
    "\n",
    "# define the numerical and categorical features\n",
    "numerical_features = ['age', 'fare']\n",
    "categorical_features = ['pclass', 'sex', 'embarked']\n",
    "\n",
    "# create the transformer for numerical and categorical features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median'))\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# create the preprocessor \n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# create the pipeline\n",
    "pipline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestClassifier(random_state=0))\n",
    "])\n",
    "\n",
    "# train the model \n",
    "pipline.fit(X_train, y_train)\n",
    "\n",
    "# predicting values\n",
    "y_pred = pipline.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaination:\n",
    "\n",
    "In this example, we start by loading the Titanic dataset from Seaborn using sns.load_dataset('titanic'). We then select the relevant features and target variable (survived) to train our model. Next, we split the data into training and test sets using train_test_split from scikit-learn.\n",
    "\n",
    "The pipeline is created using the Pipeline class from scikit-learn. It consists of three steps:\n",
    "\n",
    "`Data preprocessing step:` The SimpleImputer is used to handle missing values by replacing them with the most frequent value in each column.\n",
    "\n",
    "`Feature encoding step:` The OneHotEncoder is used to encode categorical variables (sex and embarked) as binary features.\n",
    "\n",
    "`Model training step:` The RandomForestClassifier is used as the machine learning model for classification.\n",
    "\n",
    ">We then fit the pipeline on the training data using pipeline.fit(X_train, y_train). Afterward, we make predictions on the test data using pipeline.predict(X_test).\n",
    "\n",
    "Finally, we calculate the accuracy score by comparing the predicted values (y_pred) with the actual values (y_test).\n",
    "\n",
    "Note that you may need to install Seaborn (`pip install seaborn`) if it's not already installed in your environment.\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Hyperparamter tunning in pipeline\n",
    "\n",
    "Hyperparameter tuning in a pipeline involves optimizing the hyperparameters of the different steps in the pipeline to find the best combination that maximizes the model's performance. Here's an example of hyperparameter tuning in a pipeline and selecting the best model on the Titanic dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8156424581005587\n",
      "Best hyperparameters: {'model__max_depth': None, 'model__min_samples_split': 5, 'model__n_estimators': 300}\n",
      "Best score: 0.8202600216684723\n"
     ]
    }
   ],
   "source": [
    "# import all the nessesary libraries \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# load the dataset \n",
    "titanic_data = sns.load_dataset('titanic')\n",
    "\n",
    "# selecting the features and target or label variables\n",
    "# These are the features that we will use to train the model \n",
    "X = titanic_data[['pclass','sex', 'age', 'fare', 'embarked']]\n",
    "\n",
    "# This is the label or target variable that we want to predict\n",
    "y = titanic_data['survived']\n",
    "\n",
    "# spliting the dataset into training and testing set\n",
    "X_train , X_test, y_train , y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# creating the pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ('model', RandomForestClassifier(random_state=0))\n",
    "])\n",
    "\n",
    "# defining the hyperparameters\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200, 300, 500],\n",
    "    'model__max_depth': [None, 5, 10, 20],\n",
    "    'model__min_samples_split': [2, 5, 10, 15]\n",
    "}\n",
    "\n",
    "# creating the gird search object \n",
    "gird_search = GridSearchCV(estimator= pipeline, param_grid= param_grid, cv= 5)\n",
    "\n",
    "# fitting the model with the training data features and target or label variable \n",
    "gird_search.fit(X_train, y_train)\n",
    "\n",
    "# predicting the model \n",
    "y_pred = gird_search.predict(X_test)\n",
    "\n",
    "# evaluating the model \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "# print the best hyperparameters\n",
    "print(\"Best hyperparameters:\", gird_search.best_params_)\n",
    "print(\"Best score:\", gird_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "\n",
    "# Selecting best model in Pipeline\n",
    "\n",
    "To select the best model when using multiple models in a pipeline, you can use techniques like cross-validation and evaluation metrics to compare their performance. Here's an example of how to accomplish this on the Titanic dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Gradient Boosting\n",
      "Cross-validation Accuracy: 0.8061952132374668\n",
      "Test Accuracy: 0.7988826815642458\n",
      "\n",
      "Best Model: Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n",
      "                ('encoder', OneHotEncoder(handle_unknown='ignore')),\n",
      "                ('model', GradientBoostingClassifier(random_state=42))])\n"
     ]
    }
   ],
   "source": [
    "# import all the nessesary libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "\n",
    "# load the dataset\n",
    "titanic_data = sns.load_dataset('titanic')\n",
    "\n",
    "# selecting the features and target or label variables\n",
    "# These are the features that we will use to train the model\n",
    "X = titanic_data[['pclass', 'sex', 'age', 'fare', 'embarked']]\n",
    "\n",
    "# this is the label or target variable that we want to predict \n",
    "y = titanic_data['survived']\n",
    "\n",
    "# spliting the dataset into training and testing set\n",
    "X_train , X_test, y_train , y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# creating the list on models\n",
    "models = [\n",
    "    ('Random Forest', RandomForestClassifier(random_state=42)),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(random_state=42))\n",
    "]\n",
    "best_model = None\n",
    "best_accuracy = 0\n",
    "\n",
    "# create a loop to train the model with a pipeline \n",
    "for name, model in models:\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore')),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "# perform cross validation \n",
    "score = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "# calculate the mean accuracy\n",
    "mean_accuracy = score.mean()\n",
    "\n",
    "# pipeline fit the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# predict the model\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the performance metrics\n",
    "print(\"Model:\", name)\n",
    "print(\"Cross-validation Accuracy:\", mean_accuracy)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "print()\n",
    "    \n",
    "# Check if the current model has the best accuracy\n",
    "if accuracy > best_accuracy:\n",
    "    best_accuracy = accuracy\n",
    "    best_model = pipeline\n",
    "\n",
    "# Retrieve the best model\n",
    "print(\"Best Model:\", best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we initialize the best_model and best_accuracy variables to track the best-performing model.\n",
    "\n",
    "During the iteration over the models, after calculating the accuracy score for each model, we compare it with the current best_accuracy value. If the current model has a higher accuracy, we update best_accuracy and assign the pipeline object to best_model.\n",
    "\n",
    "After the loop, we print the best model using print(\"Best Model:\", best_model).\n",
    "\n",
    "By comparing the accuracy scores of different models within the pipeline and selecting the one with the highest accuracy, you can retrieve the best-performing model for the given dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add more models in the same code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression\n",
      "Cross-validation Accuracy: 0.7977839062346105\n",
      "Test Accuracy: 0.8100558659217877\n",
      "Best Model: Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n",
      "                ('encoder', OneHotEncoder(handle_unknown='ignore')),\n",
      "                ('model', LogisticRegression(random_state=42))])\n"
     ]
    }
   ],
   "source": [
    "# Import all the nessesary libraries\n",
    "\n",
    "# these are the basic libraries that we need to import\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# these are the libraries that we need to preprocess the data\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# these are the libraries that we need to use machine learning models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# these are the libraries that we need to evaluate the model \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "\n",
    "# load the dataset from seaborn \n",
    "titani_data = sns.load_dataset('titanic')\n",
    "\n",
    "# Selecting the features and targeted labels from the data\n",
    "\n",
    "# feature variables which we use to train the model \n",
    "X = titanic_data[['pclass', 'sex', 'age', 'fare', 'embarked']]\n",
    "\n",
    "# targeted label which we want to predict \n",
    "y = titanic_data['survived']\n",
    "\n",
    "# spliting the data into training and testing set\n",
    "X_train , X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# creating the list of models which we want to train and evaluate \n",
    "models = [\n",
    "    ('Random Forest', RandomForestClassifier(random_state=42)),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(random_state=42)),\n",
    "    ('Support Vector Machine', SVC(random_state=42)),\n",
    "    ('Logistic Regression', LogisticRegression(random_state=42))\n",
    "]\n",
    "\n",
    "best_model = None\n",
    "best_accuracy = 0 \n",
    "\n",
    "# creating a loop and appling pipeline to all the models to train them \n",
    "for name, model in models:\n",
    "    pipeline = Pipeline(steps= [\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore')),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "# perform cross validation \n",
    "score = cross_val_score(pipeline, X_train, y_train, cv= 5, scoring='accuracy')\n",
    "\n",
    "# calculate the mean accuracy of the model\n",
    "mean_accuracy = score.mean()\n",
    "\n",
    "# pipeline fit the model \n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# predict the model\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# evaluate the model \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Model:\", name)\n",
    "print(\"Cross-validation Accuracy:\", mean_accuracy)  \n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "if accuracy > best_accuracy:\n",
    "    best_accuracy = accuracy\n",
    "    best_model = pipeline\n",
    "print(\"Best Model:\", best_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
