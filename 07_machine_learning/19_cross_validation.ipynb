{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "\n",
    "+ Cross validation is a technique to evaluate a model by splitting the data into a training and a test set.\n",
    "\n",
    "+ The training set is used to train the model, while the test set is used to evaluate the model's performance.\n",
    "\n",
    "Cross-validation is a statistical technique used in machine learning and data analysis to evaluate how well a model is able to generalize to new data.\n",
    "\n",
    "In a prediction problem, a model is usually given a dataset of known data on which training is run (training dataset), and a dataset of unknown data (or first seen data) against which the model is tested (called the validation dataset or testing set).\n",
    "\n",
    "The basic idea is to divide the available data into two parts:\n",
    "\n",
    "- a training set, which is used to train the model, and\n",
    "- a validation set, which is used to evaluate the model's performance.\n",
    "\n",
    "### Types of CV:\n",
    "\n",
    "1. k-fold cross-validation\n",
    "2. train-test split\n",
    "3. leave-one-out cross-validation (LOOCV)\n",
    "4. stratified cross-validation\n",
    "5. Times series cross-validation\n",
    "6. Group cross-validation\n",
    "\n",
    "## Example of K-fold cross validation\n",
    "\n",
    "In k-fold cross-validation, the available data is divided into k equal parts or \"folds\". The model is then trained on k-1 of the folds and validated on the remaining fold. This process is repeated k times, with each fold being used once as the validation set. The results from each fold are then averaged to obtain an overall estimate of the model's performance.\n",
    "### Summary:\n",
    "\n",
    "In summary, cross-validation is a powerful technique for evaluating the performance of machine learning models and can help to ensure that our models are accurate, reliable, and able to generalize to new data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of each fold:  [0.93333333 0.96666667 0.93333333 0.93333333 1.        ]\n",
      "Mean score:  0.9533333333333334\n",
      "Standard deviation:  0.02666666666666666\n",
      "CPU times: total: 46.9 ms\n",
      "Wall time: 40.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# load the dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# define the model\n",
    "nb = GaussianNB()\n",
    "\n",
    "# perform cross-validation \n",
    "scores = cross_val_score(nb, iris.data, iris.target, cv=5, scoring='accuracy')\n",
    "\n",
    "# print the scores \n",
    "print(\"Score of each fold: \", scores)\n",
    "print(\"Mean score: \", scores.mean())\n",
    "print(\"Standard deviation: \", scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold Cross Validation vs. train_test split\n",
    "\n",
    "K-fold cross-validation and train-test split are two popular techniques used in machine learning to evaluate the performance of a model. Here are some key differences between the two:\n",
    "1. **Data usage:**\n",
    "> In k-fold cross-validation, the data is split into k equal parts or \"folds\". The model is trained on k-1 of the folds and validated on the remaining fold. This process is repeated k times, with each fold being used once as the validation set.\n",
    "\n",
    ">In contrast, train-test split divides the data into two parts: a training set and a testing set, typically with a ratio of 70-30 or 80-20. The model is trained on the training set and evaluated on the testing set.\n",
    "\n",
    "2. **Data size:**\n",
    ">K-fold cross-validation is often used when the dataset is relatively small, as it allows for better use of the available data.\n",
    "\n",
    ">In contrast, train-test split is typically used when the dataset is larger, as it is faster to implement and may be sufficient for evaluating the model's performance.\n",
    "\n",
    "3. **Performance estimation:**\n",
    ">K-fold cross-validation provides a more accurate estimate of the model's performance, as it evaluates its performance on multiple independent subsets of the data. This helps to reduce the variance of the performance estimate and detect overfitting.\n",
    "\n",
    ">In contrast, train-test split provides a less accurate estimate of the model's performance, as it depends on the specific subset of the data used for testing.\n",
    "\n",
    "4. **Computation time:**\n",
    "\n",
    ">K-fold cross-validation can be computationally expensive, as it requires training and validating the model k times.\n",
    "\n",
    ">In contrast, train-test split is faster to implement and requires training and validating the model only once.\n",
    "\n",
    "Overall, `k-fold cross-validation is a more robust and accurate technique for evaluating the performance of a machine learning model, especially when the dataset is relatively small.`\n",
    "\n",
    "`Train-test split is a faster and simpler technique that can be used when the dataset is larger and a quick estimate of the model's performance is needed.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "# load the dataset \n",
    "tips = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/tips.csv')\n",
    "\n",
    "# define the feature matrix X and the target vector y\n",
    "X = tips[['total_bill', 'tip', 'size']]\n",
    "y = tips['sex']\n",
    "\n",
    "# define the gaussian naive bayes model \n",
    "model = GaussianNB()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
